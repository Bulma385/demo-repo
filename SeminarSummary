# Title: Survey Paper on ..

important topics: sequence serialization, input embedding, encoding decoding arch, attention, model efficiency

## Abstract

after success of transformers for NLP -> new SOTA achieved in structured tabular domain
hence development from Table2Vec, CNNs, RNNs, GNNs, LSTMs to Transormer based models
we are evaluating different pre-training paradigms and architectures 
& how they perform on various down-stream tasks for table understanding as table question-answering, cell filling, entity linking etc.


## Introduction
same stuff as abstract but more sources
and outlook on the results and approaches

## Preliminaries? 
why structured data difficult
datasets: web tables, database tables ...

## Main Part: Related Work: Model Architectures, Pre-Training and Fine-Tuning

### Existing Models and general approaches
Non transformer-based:
Table2Vec, CNNs, RNNs, LSTMs

Transformer-based:
TURL
TABERT
TAPAS
TABBIE

### Input Embedding

### Encoder Decoder Architecture

### Pretraining

### Downstream Tasks and task-specific Fine-Tuning
